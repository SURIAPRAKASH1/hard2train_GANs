import torch 
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

import sys 
import os 
import time
import importlib
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))) 

from common.argfile import get_args

# command line args
args = get_args()

# current device
device = "cuda" if torch.cuda.is_available() else "cpu"
print("current device -->", device)

# image shape (H, W, C) -> (height, width, channels)
H: int = args.height
W: int = args.width
C: int = args.channels
image_shape = H * W * C

################################  Generative Adversarial Network - GAN ############################################

class Generator(nn.Module):
    
    """
    Generator Model:

    Takes random Noise and Trying to models the distripution of the real data.
    In simple term, generator trying generate fake image that so good even discriminator can't tell the different

    """

    def __init__(self, noise_dim: int, out_features: int):
        super().__init__()

        """
        Args:

            noise_dim (int): Shape of noise , Usually 100 dim vector
            out_features (int): Output size . good if we use same shape as training image

        """

        # layer 1
        self.l1 = nn.Sequential(
            nn.Linear(noise_dim, 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(negative_slope= 0.2)
        )

        # layer 2
        self.l2 = nn.Sequential(
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.LeakyReLU(negative_slope= 0.2) ,
        )

        # layer 3
        self.l3 = nn.Sequential(
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.LeakyReLU(negative_slope= 0.2)
        )

        # finall layer
        self.final = nn.Sequential(
            nn.Linear(128, out_features),
        )

        # report parameters
        print(f"{self.__class__.__name__} Model parameters: {(self._get_parameters_count()/1e+6):2f}M")

    def _get_parameters_count(self)-> int:
        """
        Returns Parameters Count
        """
        t = 0
        for p in self.parameters():
            t += p.nelement()
        return t

    def forward(self, x: torch.Tensor)-> torch.Tensor:
        """
        Args:
            x (torch.Tensor) : Noise with shape (B, 100) usually
        
        Returns:
            out (torch.Tensor): Generated Image with shape (B, H, W, C)

        """

        out = self.final(self.l3(self.l2(self.l1(x))))
        # reshape to same shape as image
        out = out.view(out.size(0), H, W, C)  # (B, H, W, C)
        return out

class Discriminator(nn.Module):
    """
    Discriminator Model:

    Tells which image is Fake and which image is Real.
    Finding differnece between fake image generated by Generator  and Real Image from dataset

    """
    def __init__(self, in_features: int = (H * W * C)):
        super().__init__()

        """
        Args:

            in_features (int): Dimentionality of Image shape eg: (H, W, C)

        """

        # layer 1
        self.l1 = nn.Sequential(
            nn.Flatten(start_dim=1),
            nn.Linear(in_features, 256),
            # nn.BatchNorm1d(256),
            nn.LeakyReLU(negative_slope= 0.2),
            nn.Dropout1d(p= 0.1)
        )

        # layer 2
        self.l2 = nn.Sequential(
            nn.Linear(256, 128),
            # nn.BatchNorm1d(128),
            nn.LeakyReLU(negative_slope= 0.2),
            nn.Dropout1d(p= 0.5)

        )

        # layer 3
        self.l3 = nn.Sequential(
            nn.Linear(128, 64),
            # nn.BatchNorm1d(64),
            nn.LeakyReLU(negative_slope= 0.2),
            nn.Dropout1d(p= 0.5)

        )

        # final layer 
        self.final = nn.Sequential(
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

        print(f"{self.__class__.__name__} Model parameters: {(self._get_parameters_count()/1e+6):2f}M")

    def _get_parameters_count(self)-> int:
        """
        Returns Parameters Count
        """
        t = 0
        for p in self.parameters():
            t += p.nelement()
        return t

    def forward(self, x: torch.Tensor)-> torch.Tensor:
        """
        Args:
            x (torch.Tensor): Output from generator network of Real Image from dataset with shape (B, H, W, C)

        Returns:
            out (torch.Tensor): For Each Image gives score (0 to 1) How much is it Fake or Real. output shape (B, 1)

        """
        out = self.final(self.l3(self.l2(self.l1(x))))
        return out

print("Initiating Models")

# initiating models
G = Generator(args.latent_dim, image_shape).to(device)
torch.compile(G)
D = Discriminator(image_shape).to(device)
torch.compile(D)

print("Models are compiled")

# Binary Cross Entropy Loss
criterion = nn.BCELoss().to(device)

# Generator optimizer
opt_G = optim.AdamW(G.parameters(), lr = args.lr, betas= (0.5, 0.5))
# Discriminator optimier
opt_D = optim.AdamW(D.parameters(), lr = args.lr, weight_decay = 1e-1, betas = (0.5, 0.5))

# Dataset
if args.celebA:
    celebA = importlib.import_module("datasets.celebA")
    dataset = celebA.celebA_dataset
else:
    mnist = importlib.import_module("datasets.mnist")
    dataset = mnist.mnist_dataset

# Dataloader
dataloader = DataLoader(dataset, 
                            batch_size = args.batch_size, 
                            pin_memory= True, 
                            shuffle= True,
                            drop_last= True)

# Optimization Loop

start = time.time()
for epoch in range(args.epochs):

    if (epoch % args.print_interval == 0 or epoch == args.epochs - 1):
        print("Epoch", epoch + 1)

    for batch, (real_imgs, _) in enumerate(dataloader, 0):
        real_imgs = real_imgs.to(device, non_blocking = True)

        for _ in range(1):

            # A.Train Discriminator
            G.requires_grad_(False)
            D.requires_grad_(True)
            opt_D.zero_grad()

            # Real Images
            real_labels = torch.ones(args.batch_size, 1, device = device) - 0.01
            pred_real = D(real_imgs)
            loss_real = criterion(pred_real, real_labels)

            # Fake Images
            noise = torch.rand((args.batch_size, args.latent_dim), device = device) * 2 - 1
            fake_labels = torch.zeros(args.batch_size, 1, device = device)
            fake_imgs = G(noise).detach()     # make sure we don't train G when traning D
            pred_fake = D(fake_imgs)
            loss_fake = criterion(pred_fake, fake_labels)

            # back-prop and update D parameters
            loss_D = (loss_real + loss_fake) * 0.5
            loss_D.backward()
            opt_D.step()

        # B.Train Generator
        G.requires_grad_(True)
        D.requires_grad_(False)
        opt_G.zero_grad()

        noise = torch.rand((args.batch_size, args.latent_dim), device = device) * 2 - 1
        generated = G(noise)
        # fool the D to think it's reciving real images
        target_labels = torch.ones(args.batch_size, 1, device = device) - 0.1
        pred = D(generated)
        loss_G = criterion(pred, target_labels)

        # back-prop and update G parameters
        loss_G.backward()
        opt_G.step()

        if (epoch % args.print_interval == 0 or epoch == args.epochs - 1) and ( batch % 200 == 0 or batch == dataloader.__len__()-1):
            print(f"{dataloader.__len__()}/{batch}: D: loss_real {loss_real}, loss_fake {loss_fake} G: loss {loss_G}")

end = time.time()
print("training time %.2f" % ((end - start)/60),"M")
